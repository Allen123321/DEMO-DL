{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "30_days_deep_learning_20.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNEFtMHAi9Qpzqd2BKtVzE3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Allen123321/DEMO-DL/blob/master/30_days_deep_learning_20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BVlCtoRcA_t"
      },
      "source": [
        "## 6-3,使用单GPU训练模型\r\n",
        "深度学习的训练过程常常非常耗时，一个模型训练几个小时是家常便饭，训练几天也是常有的事情，有时候甚至要训练几十天。<br>\r\n",
        "训练过程的耗时主要来自于两个部分，一部分来自数据准备，另一部分来自参数迭代。<br>\r\n",
        "当数据准备过程还是模型训练时间的主要瓶颈时，我们可以使用更多进程来准备数据。<br>\r\n",
        "当参数迭代过程成为训练时间的主要瓶颈时，我们通常的方法是应用GPU或者Google的TPU来进行加速。<br>\r\n",
        "无论是内置fit方法，还是自定义训练循环，从CPU切换成单GPU训练模型都是非常方便的，无需更改任何代码。当存在可用的GPU时，如果不特意指定device，tensorflow会自动优先选择使用GPU来创建张量和执行张量计算。<br>\r\n",
        "但如果是在公司或者学校实验室的服务器环境，存在多个GPU和多个使用者时，为了不让单个同学的任务占用全部GPU资源导致其他同学无法使用（tensorflow默认获取全部GPU的全部内存资源权限，但实际上只使用一个GPU的部分资源），我们通常会在开头增加以下几行代码以控制每个任务使用的GPU编号和显存大小，以便其他同学也能够同时训练模型。<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XRiS59WYYFV",
        "outputId": "597cabb9-607f-4a39-ab0d-5993fffcfdbf"
      },
      "source": [
        "%tensorflow_version 2.x\r\n",
        "import tensorflow as tf\r\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78vnqttLdagB"
      },
      "source": [
        "from tensorflow.keras import * \r\n",
        "\r\n",
        "#打印时间分割线\r\n",
        "@tf.function\r\n",
        "def printbar():\r\n",
        "    today_ts = tf.timestamp()%(24*60*60)\r\n",
        "\r\n",
        "    hour = tf.cast(today_ts//3600+2,tf.int32)%tf.constant(24)\r\n",
        "    minite = tf.cast((today_ts%3600)//60,tf.int32)\r\n",
        "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\r\n",
        "    \r\n",
        "    def timeformat(m):\r\n",
        "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\r\n",
        "            return(tf.strings.format(\"0{}\",m))\r\n",
        "        else:\r\n",
        "            return(tf.strings.format(\"{}\",m))\r\n",
        "    \r\n",
        "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\r\n",
        "                timeformat(second)],separator = \":\")\r\n",
        "    tf.print(\"==========\"*8+timestring)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU_b3hSQdhW5"
      },
      "source": [
        "## 一、GPU设置\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yfiVlCXdg0G"
      },
      "source": [
        "gpus = tf.config.list_physical_devices(\"GPU\")\r\n",
        "if gpus:\r\n",
        "    gpu0 = gpus[0] #如果有多个GPU，仅使用第0个GPU\r\n",
        "    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用\r\n",
        "    # 或者也可以设置GPU显存为固定使用量(例如：4G)\r\n",
        "    #tf.config.experimental.set_virtual_device_configuration(gpu0,\r\n",
        "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) \r\n",
        "    tf.config.set_visible_devices([gpu0],\"GPU\") "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNea2V3peEKV"
      },
      "source": [
        "比较GPU和CPU的计算速度"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaY1OUpfeDR_",
        "outputId": "8061a17b-cd06-409c-d044-af25e704bd66"
      },
      "source": [
        "printbar()\r\n",
        "with tf.device(\"/gpu:0\"):\r\n",
        "    tf.random.set_seed(0)\r\n",
        "    a = tf.random.uniform((10000,100),minval = 0,maxval = 3.0)\r\n",
        "    b = tf.random.uniform((100,100000),minval = 0,maxval = 3.0)\r\n",
        "    c = a@b\r\n",
        "    tf.print(tf.reduce_sum(tf.reduce_sum(c,axis = 0),axis=0))\r\n",
        "printbar()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================15:25:47\n",
            "2.24953778e+11\n",
            "================================================================================15:25:49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z-w7sTneXkF",
        "outputId": "ce319b86-0cec-4f9a-927e-dff556cc494a"
      },
      "source": [
        "printbar()\r\n",
        "with tf.device(\"/cpu:0\"):\r\n",
        "    tf.random.set_seed(0)\r\n",
        "    a = tf.random.uniform((10000,100),minval = 0,maxval = 3.0)\r\n",
        "    b = tf.random.uniform((100,100000),minval = 0,maxval = 3.0)\r\n",
        "    c = a@b\r\n",
        "    tf.print(tf.reduce_sum(tf.reduce_sum(c,axis = 0),axis=0))\r\n",
        "printbar()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================15:26:10\n",
            "2.24953778e+11\n",
            "================================================================================15:26:17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArLl5Owze9eu"
      },
      "source": [
        "## 二，准备数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Qkz-SdMe8Iw",
        "outputId": "9e8279c7-1432-42fc-f402-400a34569892"
      },
      "source": [
        "MAX_LEN = 300\r\n",
        "BATCH_SIZE = 32\r\n",
        "(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()\r\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)\r\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)\r\n",
        "\r\n",
        "MAX_WORDS = x_train.max()+1\r\n",
        "CAT_NUM = y_train.max()+1\r\n",
        "\r\n",
        "ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \\\r\n",
        "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\r\n",
        "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\r\n",
        "   \r\n",
        "ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \\\r\n",
        "          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\r\n",
        "          .prefetch(tf.data.experimental.AUTOTUNE).cache()\r\n",
        "          "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XqWxtgjfe1c"
      },
      "source": [
        "## 三，定义模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFc0VOgffc3r",
        "outputId": "13aca673-bfac-41b2-ba0a-2048cd45d620"
      },
      "source": [
        "tf.keras.backend.clear_session()\r\n",
        "\r\n",
        "def create_model():\r\n",
        "    \r\n",
        "    model = models.Sequential()\r\n",
        "\r\n",
        "    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\r\n",
        "    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\r\n",
        "    model.add(layers.MaxPool1D(2))\r\n",
        "    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\r\n",
        "    model.add(layers.MaxPool1D(2))\r\n",
        "    model.add(layers.Flatten())\r\n",
        "    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\r\n",
        "    return(model)\r\n",
        "\r\n",
        "model = create_model()\r\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 300, 7)            216874    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2336)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 46)                107502    \n",
            "=================================================================\n",
            "Total params: 332,856\n",
            "Trainable params: 332,856\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw2WhszcggeA"
      },
      "source": [
        "## 四，训练模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnMiYBpEgf8l",
        "outputId": "b0f1c507-e479-4619-ac90-71be65cbb7ca"
      },
      "source": [
        "optimizer = optimizers.Nadam()\r\n",
        "loss_func = losses.SparseCategoricalCrossentropy()\r\n",
        "\r\n",
        "train_loss = metrics.Mean(name='train_loss')\r\n",
        "train_metric = metrics.SparseCategoricalAccuracy(name='train_accuracy')\r\n",
        "\r\n",
        "valid_loss = metrics.Mean(name='valid_loss')\r\n",
        "valid_metric = metrics.SparseCategoricalAccuracy(name='valid_accuracy')\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def train_step(model, features, labels):\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "        predictions = model(features,training = True)\r\n",
        "        loss = loss_func(labels, predictions)\r\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\r\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n",
        "\r\n",
        "    train_loss.update_state(loss)\r\n",
        "    train_metric.update_state(labels, predictions)\r\n",
        "    \r\n",
        "@tf.function\r\n",
        "def valid_step(model, features, labels):\r\n",
        "    predictions = model(features)\r\n",
        "    batch_loss = loss_func(labels, predictions)\r\n",
        "    valid_loss.update_state(batch_loss)\r\n",
        "    valid_metric.update_state(labels, predictions)\r\n",
        "    \r\n",
        "\r\n",
        "def train_model(model,ds_train,ds_valid,epochs):\r\n",
        "    for epoch in tf.range(1,epochs+1):\r\n",
        "        \r\n",
        "        for features, labels in ds_train:\r\n",
        "            train_step(model,features,labels)\r\n",
        "\r\n",
        "        for features, labels in ds_valid:\r\n",
        "            valid_step(model,features,labels)\r\n",
        "\r\n",
        "        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}'\r\n",
        "        \r\n",
        "        if epoch%1 ==0:\r\n",
        "            printbar()\r\n",
        "            tf.print(tf.strings.format(logs,\r\n",
        "            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\r\n",
        "            tf.print(\"\")\r\n",
        "            \r\n",
        "        train_loss.reset_states()\r\n",
        "        valid_loss.reset_states()\r\n",
        "        train_metric.reset_states()\r\n",
        "        valid_metric.reset_states()\r\n",
        "\r\n",
        "train_model(model,ds_train,ds_test,10)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================15:35:50\n",
            "Epoch=1,Loss:2.00793815,Accuracy:0.471387208,Valid Loss:1.67046547,Valid Accuracy:0.567230642\n",
            "\n",
            "================================================================================15:35:51\n",
            "Epoch=2,Loss:1.47146297,Accuracy:0.618347824,Valid Loss:1.51847458,Valid Accuracy:0.627782702\n",
            "\n",
            "================================================================================15:35:53\n",
            "Epoch=3,Loss:1.16430438,Accuracy:0.699621439,Valid Loss:1.52111256,Valid Accuracy:0.643366\n",
            "\n",
            "================================================================================15:35:54\n",
            "Epoch=4,Loss:0.880724907,Accuracy:0.772545099,Valid Loss:1.68295872,Valid Accuracy:0.645146906\n",
            "\n",
            "================================================================================15:35:56\n",
            "Epoch=5,Loss:0.647900343,Accuracy:0.837564,Valid Loss:1.94253516,Valid Accuracy:0.640249312\n",
            "\n",
            "================================================================================15:35:57\n",
            "Epoch=6,Loss:0.487701416,Accuracy:0.880093515,Valid Loss:2.20510244,Valid Accuracy:0.642920732\n",
            "\n",
            "================================================================================15:35:59\n",
            "Epoch=7,Loss:0.389409,Accuracy:0.906034291,Valid Loss:2.33843875,Valid Accuracy:0.642030299\n",
            "\n",
            "================================================================================15:36:00\n",
            "Epoch=8,Loss:0.328082442,Accuracy:0.92273438,Valid Loss:2.44059134,Valid Accuracy:0.642920732\n",
            "\n",
            "================================================================================15:36:02\n",
            "Epoch=9,Loss:0.286933035,Accuracy:0.931752384,Valid Loss:2.51350474,Valid Accuracy:0.644256473\n",
            "\n",
            "================================================================================15:36:03\n",
            "Epoch=10,Loss:0.256004035,Accuracy:0.937430441,Valid Loss:2.5904,Valid Accuracy:0.638468385\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}