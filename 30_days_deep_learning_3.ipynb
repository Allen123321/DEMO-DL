{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "30_days_deep_learning_3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMaHBHv3hLOTv1KiNNmX7bK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Allen123321/DEMO-DL/blob/master/30_days_deep_learning_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHhRN5LUtBme",
        "outputId": "9b1281df-4799-4e1d-d407-c434b2ab6527"
      },
      "source": [
        "from google.colab import drive  #挂载drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFtZvBSHxCvR",
        "outputId": "6a9682c9-4b42-4f70-bf5d-5b219bb74bc4"
      },
      "source": [
        "!unzip -o /content/Test.csv.zip # linux 解压命令"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/Test.csv.zip\n",
            "  inflating: Test.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZdYnSiexSKZ",
        "outputId": "481313f7-aa49-46d6-da92-f043aad79431"
      },
      "source": [
        "\r\n",
        "!unzip -o /content/Valid.csv.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/Train.csv.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/Train.csv.zip or\n",
            "        /content/Train.csv.zip.zip, and cannot find /content/Train.csv.zip.ZIP, period.\n",
            "Archive:  /content/Valid.csv.zip\n",
            "  inflating: Valid.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fI2jBKBxiXD",
        "outputId": "2d49aad1-ead6-4856-bb58-3f17cbefc212"
      },
      "source": [
        "!unzip -o /content/Train.csv.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/Train.csv.zip\n",
            "  inflating: Train.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyha3tycZf_P"
      },
      "source": [
        "example-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZw9Kv7dnfku"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import models,layers,preprocessing,optimizers,losses,metrics,datasets\r\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n",
        "import re,string\r\n",
        "from tensorflow.keras.datasets import imdb\r\n",
        "from tensorflow.keras.preprocessing import sequence\r\n",
        "\r\n",
        "\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras.layers import LSTM\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMYiz92jx0l9"
      },
      "source": [
        "(train_data,train_labels),(test_data,test_labels) = imdb.load_data(num_words=10000)\r\n",
        "\r\n",
        "\r\n",
        "# train_data_path = \"/content/Train.csv\"\r\n",
        "# test_data_path =  \"/content/Test.csv\"\r\n",
        "\r\n",
        "MAX_WORDS = 10000  # 仅考虑最高频的10000个词\r\n",
        "MAX_LEN = 200  # 每个样本保留200个词的长度\r\n",
        "BATCH_SIZE = 20 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDWoppMlOdDp"
      },
      "source": [
        "train_data=sequence.pad_sequences(train_data, maxlen = MAX_LEN)\r\n",
        "test_data=sequence.pad_sequences(test_data, maxlen = MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSkoJ583RSoO",
        "outputId": "50d7573b-1b78-4be9-b92b-482e4b1bb2fa"
      },
      "source": [
        "embedding_vector_length=32\r\n",
        "model=Sequential()\r\n",
        "model.add(layers.Embedding(MAX_WORDS,embedding_vector_length,input_length=MAX_LEN))\r\n",
        "model.add(LSTM(100)) # LSTM的具体用法参考 https://keras.io/layers/recurrent/#lstm\r\n",
        "model.add(Dense(1,activation='sigmoid'))\r\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\r\n",
        "print(model.summary())\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 200, 32)           320000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 373,301\n",
            "Trainable params: 373,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y2Y_re4SIx1",
        "outputId": "23bfff1a-1d7d-48ab-80cf-641cd14a6312"
      },
      "source": [
        "model.fit(train_data,train_labels,validation_data=(test_data,test_labels),epochs=5,batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "391/391 [==============================] - 115s 294ms/step - loss: 0.4191 - accuracy: 0.8030 - val_loss: 0.3793 - val_accuracy: 0.8440\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 116s 296ms/step - loss: 0.2440 - accuracy: 0.9075 - val_loss: 0.4330 - val_accuracy: 0.8279\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 116s 298ms/step - loss: 0.1858 - accuracy: 0.9322 - val_loss: 0.3457 - val_accuracy: 0.8616\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 116s 298ms/step - loss: 0.1499 - accuracy: 0.9464 - val_loss: 0.3954 - val_accuracy: 0.8445\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 115s 295ms/step - loss: 0.1144 - accuracy: 0.9595 - val_loss: 0.4001 - val_accuracy: 0.8604\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fac530f77b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgNvLqW0XULx"
      },
      "source": [
        "example - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4IwGI-Jo49m"
      },
      "source": [
        "文本数据建模流程范例\r\n",
        "\r\n",
        "imdb数据集的目标是根据电影评论的文本内容预测评论的情感标签。\r\n",
        "\r\n",
        "训练集有20000条电影评论文本，测试集有5000条电影评论文本，其中正面评论和负面评论都各占一半。\r\n",
        "\r\n",
        "文本数据预处理较为繁琐，包括中文切词（本示例不涉及），构建词典，编码转换，序列填充，构建数据管道等等。\r\n",
        "\r\n",
        "在tensorflow中完成文本数据预处理的常用方案有两种，第一种是利用tf.keras.preprocessing中的Tokenizer词典构建工具和tf.keras.utils.Sequence构建文本数据生成器管道。\r\n",
        "\r\n",
        "第二种是使用tf.data.Dataset搭配.keras.layers.experimental.preprocessing.TextVectorization预处理层。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUdPQhzZS0J0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc14103-0446-49c9-dea3-1a53ca659299"
      },
      "source": [
        "import numpy as np \r\n",
        "import pandas as pd \r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import models,layers,preprocessing,optimizers,losses,metrics\r\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n",
        "import re,string\r\n",
        "\r\n",
        "train_data_path = \"/content/train.csv\"\r\n",
        "test_data_path =  \"/content/test.csv\"\r\n",
        "\r\n",
        "MAX_WORDS = 10000  # 仅考虑最高频的10000个词\r\n",
        "MAX_LEN = 200  # 每个样本保留200个词的长度\r\n",
        "BATCH_SIZE = 20 \r\n",
        "\r\n",
        "\r\n",
        "#构建管道\r\n",
        "def split_line(line):\r\n",
        "    arr = tf.strings.split(line,\"\\t\")\r\n",
        "    label = tf.expand_dims(tf.cast(tf.strings.to_number(arr[0]),tf.int32),axis = 0)\r\n",
        "    text = tf.expand_dims(arr[1],axis = 0)\r\n",
        "    return (text,label)\r\n",
        "\r\n",
        "ds_train_raw =  tf.data.TextLineDataset(filenames = [train_data_path]) \\\r\n",
        "   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \\\r\n",
        "   .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\r\n",
        "   .prefetch(tf.data.experimental.AUTOTUNE)\r\n",
        "\r\n",
        "ds_test_raw = tf.data.TextLineDataset(filenames = [test_data_path]) \\\r\n",
        "   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \\\r\n",
        "   .batch(BATCH_SIZE) \\\r\n",
        "   .prefetch(tf.data.experimental.AUTOTUNE)\r\n",
        "\r\n",
        "\r\n",
        "#构建词典\r\n",
        "def clean_text(text):\r\n",
        "    lowercase = tf.strings.lower(text)\r\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\r\n",
        "    cleaned_punctuation = tf.strings.regex_replace(stripped_html,\r\n",
        "         '[%s]' % re.escape(string.punctuation),'')\r\n",
        "    return cleaned_punctuation\r\n",
        "\r\n",
        "vectorize_layer = TextVectorization(\r\n",
        "    standardize=clean_text,\r\n",
        "    split = 'whitespace',\r\n",
        "    max_tokens=MAX_WORDS-1, #有一个留给占位符\r\n",
        "    output_mode='int',\r\n",
        "    output_sequence_length=MAX_LEN)\r\n",
        "\r\n",
        "ds_text = ds_train_raw.map(lambda text,label: text)\r\n",
        "vectorize_layer.adapt(ds_text)\r\n",
        "print(vectorize_layer.get_vocabulary()[0:100])\r\n",
        "\r\n",
        "\r\n",
        "#单词编码\r\n",
        "ds_train = ds_train_raw.map(lambda text,label:(vectorize_layer(text),label)) \\\r\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\r\n",
        "ds_test = ds_test_raw.map(lambda text,label:(vectorize_layer(text),label)) \\\r\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i', 'this', 'that', 'was', 'as', 'for', 'with', 'movie', 'but', 'film', 'on', 'you', 'not', 'are', 'his', 'have', 'be', 'he', 'one', 'its', 'all', 'at', 'by', 'an', 'they', 'from', 'who', 'so', 'like', 'her', 'or', 'just', 'about', 'has', 'if', 'out', 'some', 'there', 'what', 'good', 'when', 'more', 'very', 'even', 'no', 'would', 'she', 'my', 'up', 'which', 'time', 'story', 'only', 'their', 'really', 'had', 'were', 'can', 'see', 'me', 'we', 'than', 'well', 'much', 'get', 'been', 'will', 'people', 'also', 'into', 'because', 'do', 'bad', 'other', 'great', 'how', 'first', 'him', 'most', 'dont', 'made', 'then', 'them', 'films', 'could', 'make', 'way', 'movies', 'after', 'too']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KG0346guXwnP"
      },
      "source": [
        "二，定义模型\r\n",
        "\r\n",
        "使用Keras接口有以下3种方式构建模型：使用Sequential按层顺序构建模型，使用函数式API构建任意结构模型，继承Model基类构建自定义模型。\r\n",
        "\r\n",
        "\r\n",
        "此处选择使用继承Model基类构建自定义模型。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k62fR3KNXRpS",
        "outputId": "84ded409-dbb9-4861-e5ab-fa3e78e0fb01"
      },
      "source": [
        "# 演示自定义模型范例，实际上应该优先使用Sequential或者函数式API\r\n",
        "tf.keras.backend.clear_session()\r\n",
        "\r\n",
        "class CnnModel(models.Model):\r\n",
        "    def __init__(self):\r\n",
        "        super(CnnModel, self).__init__()\r\n",
        "        \r\n",
        "    def build(self,input_shape):\r\n",
        "        self.embedding = layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN)\r\n",
        "        self.conv_1 = layers.Conv1D(16, kernel_size= 5,name = \"conv_1\",activation = \"relu\")\r\n",
        "        self.pool_1 = layers.MaxPool1D(name = \"pool_1\")\r\n",
        "        self.conv_2 = layers.Conv1D(128, kernel_size=2,name = \"conv_2\",activation = \"relu\")\r\n",
        "        self.pool_2 = layers.MaxPool1D(name = \"pool_2\")\r\n",
        "        self.flatten = layers.Flatten()\r\n",
        "        self.dense = layers.Dense(1,activation = \"sigmoid\")\r\n",
        "        super(CnnModel,self).build(input_shape)\r\n",
        "    \r\n",
        "    def call(self, x):\r\n",
        "        x = self.embedding(x)\r\n",
        "        x = self.conv_1(x)\r\n",
        "        x = self.pool_1(x)\r\n",
        "        x = self.conv_2(x)\r\n",
        "        x = self.pool_2(x)\r\n",
        "        x = self.flatten(x)\r\n",
        "        x = self.dense(x)\r\n",
        "        return(x)\r\n",
        "    \r\n",
        "    # 用于显示Output Shape\r\n",
        "    def summary(self):\r\n",
        "        x_input = layers.Input(shape = MAX_LEN)\r\n",
        "        output = self.call(x_input)\r\n",
        "        model = tf.keras.Model(inputs = x_input,outputs = output)\r\n",
        "        model.summary()\r\n",
        "    \r\n",
        "model = CnnModel()\r\n",
        "model.build(input_shape =(None,MAX_LEN))\r\n",
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 200)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 200, 7)            70000     \n",
            "_________________________________________________________________\n",
            "conv_1 (Conv1D)              (None, 196, 16)           576       \n",
            "_________________________________________________________________\n",
            "pool_1 (MaxPooling1D)        (None, 98, 16)            0         \n",
            "_________________________________________________________________\n",
            "conv_2 (Conv1D)              (None, 97, 128)           4224      \n",
            "_________________________________________________________________\n",
            "pool_2 (MaxPooling1D)        (None, 48, 128)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 6144)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 6145      \n",
            "=================================================================\n",
            "Total params: 80,945\n",
            "Trainable params: 80,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGYvlHu0X79F"
      },
      "source": [
        "三，训练模型\r\n",
        "\r\n",
        "训练模型通常有3种方法，内置fit方法，内置train_on_batch方法，以及自定义训练循环。此处我们通过自定义训练循环训练模型。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYB_qqHQX6Ay",
        "outputId": "c2930e3a-7544-421c-b484-a460dcd92e24"
      },
      "source": [
        "#打印时间分割线\r\n",
        "@tf.function\r\n",
        "def printbar():\r\n",
        "    today_ts = tf.timestamp()%(24*60*60)\r\n",
        "    \r\n",
        "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\r\n",
        "    minite = tf.cast((today_ts%3600)//60,tf.int32)\r\n",
        "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\r\n",
        "    \r\n",
        "    def timeformat(m):\r\n",
        "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\r\n",
        "            return(tf.strings.format(\"0{}\",m))\r\n",
        "        else:\r\n",
        "            return(tf.strings.format(\"{}\",m))\r\n",
        "    \r\n",
        "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\r\n",
        "                timeformat(second)],separator = \":\")\r\n",
        "    tf.print(\"==========\"*8+timestring)\r\n",
        "optimizer = optimizers.Nadam()\r\n",
        "loss_func = losses.BinaryCrossentropy()\r\n",
        "\r\n",
        "train_loss = metrics.Mean(name='train_loss')\r\n",
        "train_metric = metrics.BinaryAccuracy(name='train_accuracy')\r\n",
        "\r\n",
        "valid_loss = metrics.Mean(name='valid_loss')\r\n",
        "valid_metric = metrics.BinaryAccuracy(name='valid_accuracy')\r\n",
        "\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def train_step(model, features, labels):\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "        predictions = model(features,training = True)\r\n",
        "        loss = loss_func(labels, predictions)\r\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\r\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n",
        "\r\n",
        "    train_loss.update_state(loss)\r\n",
        "    train_metric.update_state(labels, predictions)\r\n",
        "    \r\n",
        "\r\n",
        "@tf.function\r\n",
        "def valid_step(model, features, labels):\r\n",
        "    predictions = model(features,training = False)\r\n",
        "    batch_loss = loss_func(labels, predictions)\r\n",
        "    valid_loss.update_state(batch_loss)\r\n",
        "    valid_metric.update_state(labels, predictions)\r\n",
        "\r\n",
        "\r\n",
        "def train_model(model,ds_train,ds_valid,epochs):\r\n",
        "    for epoch in tf.range(1,epochs+1):\r\n",
        "        \r\n",
        "        for features, labels in ds_train:\r\n",
        "            train_step(model,features,labels)\r\n",
        "\r\n",
        "        for features, labels in ds_valid:\r\n",
        "            valid_step(model,features,labels)\r\n",
        "        \r\n",
        "        #此处logs模板需要根据metric具体情况修改\r\n",
        "        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}' \r\n",
        "        \r\n",
        "        if epoch%1==0:\r\n",
        "            printbar()\r\n",
        "            tf.print(tf.strings.format(logs,\r\n",
        "            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\r\n",
        "            tf.print(\"\")\r\n",
        "        \r\n",
        "        train_loss.reset_states()\r\n",
        "        valid_loss.reset_states()\r\n",
        "        train_metric.reset_states()\r\n",
        "        valid_metric.reset_states()\r\n",
        "\r\n",
        "train_model(model,ds_train,ds_test,epochs = 6)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================00:27:12\n",
            "Epoch=1,Loss:0.458000243,Accuracy:0.7519,Valid Loss:0.332691103,Valid Accuracy:0.8562\n",
            "\n",
            "================================================================================00:27:24\n",
            "Epoch=2,Loss:0.25274092,Accuracy:0.90055,Valid Loss:0.354468912,Valid Accuracy:0.858\n",
            "\n",
            "================================================================================00:27:36\n",
            "Epoch=3,Loss:0.181448966,Accuracy:0.932,Valid Loss:0.38288641,Valid Accuracy:0.8592\n",
            "\n",
            "================================================================================00:27:49\n",
            "Epoch=4,Loss:0.124764584,Accuracy:0.9543,Valid Loss:0.487780154,Valid Accuracy:0.8518\n",
            "\n",
            "================================================================================00:28:01\n",
            "Epoch=5,Loss:0.0811744258,Accuracy:0.97235,Valid Loss:0.5775792,Valid Accuracy:0.8554\n",
            "\n",
            "================================================================================00:28:14\n",
            "Epoch=6,Loss:0.0474837497,Accuracy:0.98505,Valid Loss:0.781416059,Valid Accuracy:0.8466\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aOBIxqdYeIH"
      },
      "source": [
        "# 四，评估模型\r\n",
        "# 通过自定义训练循环训练的模型没有经过编译，无法直接使用model.evaluate(ds_valid)方法\r\n",
        "\r\n",
        "\r\n",
        "def evaluate_model(model,ds_valid):\r\n",
        "    for features, labels in ds_valid:\r\n",
        "         valid_step(model,features,labels)\r\n",
        "    logs = 'Valid Loss:{},Valid Accuracy:{}' \r\n",
        "    tf.print(tf.strings.format(logs,(valid_loss.result(),valid_metric.result())))\r\n",
        "    \r\n",
        "    valid_loss.reset_states()\r\n",
        "    train_metric.reset_states()\r\n",
        "    valid_metric.reset_states()\r\n",
        "\r\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6TS2fbkYl-5",
        "outputId": "e73d77d8-0428-4b89-a483-2c949c92ec36"
      },
      "source": [
        "evaluate_model(model,ds_test)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Valid Loss:0.781416059,Valid Accuracy:0.8466\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL5MPGYsYp2Y"
      },
      "source": [
        "五，使用模型\r\n",
        "\r\n",
        "可以使用以下方法:\r\n",
        "model.predict(ds_test)\r\n",
        "\r\n",
        "model(x_test)\r\n",
        "\r\n",
        "model.call(x_test)\r\n",
        "\r\n",
        "model.predict_on_batch(x_test)\r\n",
        "\r\n",
        "推荐优先使用model.predict(ds_test)方法，既可以对Dataset，也可以对Tensor使用。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeguQEkQYpPf",
        "outputId": "9b1bebae-9e9b-4394-d00e-b10340f8127e"
      },
      "source": [
        "model.predict(ds_test)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.21594143],\n",
              "       [0.9999995 ],\n",
              "       [0.9929304 ],\n",
              "       ...,\n",
              "       [0.9978466 ],\n",
              "       [0.7512417 ],\n",
              "       [1.        ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHzBkEWmY7PS",
        "outputId": "09e9e9bb-ac3f-4564-ba39-1971fb297071"
      },
      "source": [
        "for x_test,_ in ds_test.take(1):\r\n",
        "    print(model(x_test))\r\n",
        "    #以下方法等价：\r\n",
        "    #print(model.call(x_test))\r\n",
        "    #print(model.predict_on_batch(x_test))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[2.1594149e-01]\n",
            " [9.9999952e-01]\n",
            " [9.9293035e-01]\n",
            " [9.8558132e-07]\n",
            " [9.8469937e-01]\n",
            " [1.7200410e-03]\n",
            " [5.0301978e-06]\n",
            " [4.6303179e-05]\n",
            " [9.9996924e-01]\n",
            " [9.9994671e-01]\n",
            " [9.9997520e-01]\n",
            " [9.9848831e-01]\n",
            " [3.4775640e-06]\n",
            " [9.8042583e-01]\n",
            " [2.0674287e-10]\n",
            " [8.4212399e-01]\n",
            " [1.4600376e-05]\n",
            " [3.2137126e-02]\n",
            " [3.7461519e-04]\n",
            " [9.8414713e-01]], shape=(20, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gpx4NPjZDlX"
      },
      "source": [
        "六，保存模型\r\n",
        "推荐使用TensorFlow原生方式保存模型。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgDWmsQIZCI-",
        "outputId": "336e8dc5-a2c0-441c-9d18-79d034b20f18"
      },
      "source": [
        "model.save('./data/tf_model_savedmodel', save_format=\"tf\")\r\n",
        "print('export saved model.')\r\n",
        "\r\n",
        "model_loaded = tf.keras.models.load_model('./data/tf_model_savedmodel')\r\n",
        "model_loaded.predict(ds_test)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: ./data/tf_model_savedmodel/assets\n",
            "export saved model.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.21594143],\n",
              "       [0.9999995 ],\n",
              "       [0.9929304 ],\n",
              "       ...,\n",
              "       [0.9978466 ],\n",
              "       [0.7512417 ],\n",
              "       [1.        ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}