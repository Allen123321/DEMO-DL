{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "用keras构建自己的网络层",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOA9W/94aHRZ64ZTkAIAYpt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Allen123321/DEMO-DL/blob/master/%E7%94%A8keras%E6%9E%84%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E7%BD%91%E7%BB%9C%E5%B1%82.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U943r6DeWrP"
      },
      "source": [
        "构建一个简单的网络层\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "我们可以通过继承tf.keras.layer.Layer,实现一个自定义的网络层。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV-ARFWyShXY"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import tensorflow as tf\n",
        "tf.keras.backend.clear_session()\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.layers as layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWojj1-ne114",
        "outputId": "629c73e2-8e4f-404d-a449-4e29f14e0831"
      },
      "source": [
        "#定义网络层就是：设置网络权重和输出到输入的计算过程\n",
        "class MyLayer(layers.Layer):\n",
        "  def __init__(self,input_dim =32,unit=32):\n",
        "    super(MyLayer,self).__init__()\n",
        "    w_init = tf.random_normal_initializer()\n",
        "\n",
        "    #权重变量\n",
        "    self.weight =tf.Variable(initial_value=w_init(\n",
        "        shape = (input_dim,unit),dtype = tf.float32),trainable=True)\n",
        "    \n",
        "    b_init = tf.zeros_initializer()\n",
        "    #偏置变量\n",
        "    self.bias = tf.Variable(initial_value=b_init(\n",
        "        shape=(unit,),dtype = tf.float32),trainable=True)\n",
        "    \n",
        "  def call(self,inputs):\n",
        "    #全连接网络\n",
        "    return tf.matmul(inputs,self.weight) + self.bias\n",
        "\n",
        "x = tf.ones((3,5))\n",
        "my_layer = MyLayer(5,4)\n",
        "out = my_layer(x)\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.00081081  0.1196294  -0.07460709  0.0345736 ]\n",
            " [ 0.00081081  0.1196294  -0.07460709  0.0345736 ]\n",
            " [ 0.00081081  0.1196294  -0.07460709  0.0345736 ]], shape=(3, 4), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKAOimW7jxf7"
      },
      "source": [
        "按上面构建网络层，图层会自动跟踪权重w和b，当然我们也可以直接用add_weight的方法构建权重"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ8bJhs5jzdW",
        "outputId": "15737930-cc6b-49c2-c00a-6a2446c7e267"
      },
      "source": [
        "class MyLayer(layers.Layer):\n",
        "  def __init__(self, input_dim=32, unit=32):\n",
        "    super(MyLayer,self).__init__()\n",
        "    # 使用add_weight添加网络变量，使其可追踪\n",
        "    self.weight = self.add_weight(shape=(input_dim, unit),\n",
        "                                  initializer=keras.initializers.RandomNormal(),\n",
        "                                  trainable=True)\n",
        "    self.bias = self.add_weight(shape=(unit,),initializer =keras.initializers.zeros(),trainable=True)\n",
        "\n",
        "  def call(self,inputs):\n",
        "    return tf.matmul(inputs,self.weight) + self.bias\n",
        "\n",
        "x = tf.ones((3,5))\n",
        "my_layer = MyLayer(5,4)\n",
        "out = my_layer(x)\n",
        "print(out)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.0652959   0.2219356  -0.03748446 -0.06070005]\n",
            " [ 0.0652959   0.2219356  -0.03748446 -0.06070005]\n",
            " [ 0.0652959   0.2219356  -0.03748446 -0.06070005]], shape=(3, 4), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-lZi0TWnIxo"
      },
      "source": [
        "也可以设置不可训练的权重"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44lSYdlAnHNn",
        "outputId": "3e1d7144-efd9-4d48-bb52-d5d49214e0bf"
      },
      "source": [
        "class AddLayer(layers.Layer):\n",
        "  def __init__(self,input_dim=32):\n",
        "    super(AddLayer,self).__init__()\n",
        "    #只存储，不训练的变量\n",
        "    self.sum = self.add_weight(shape=(input_dim,),\n",
        "                               initializer=keras.initializers.Zeros(),\n",
        "                               trainable =False)\n",
        "    \n",
        "  def call(self,inputs):\n",
        "    self.sum.assign_add(tf.reduce_sum(inputs,axis=0))\n",
        "    return self.sum\n",
        "\n",
        "x = tf.ones((3,3))\n",
        "my_layer = AddLayer(3)\n",
        "out = my_layer(x)\n",
        "print(out.numpy())\n",
        "out = my_layer(x)\n",
        "print(out.numpy())\n",
        "print('weight:', my_layer.weights)\n",
        "print('non-trainable weight:', my_layer.non_trainable_weights)\n",
        "print('trainable weight:', my_layer.trainable_weights)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3. 3. 3.]\n",
            "[6. 6. 6.]\n",
            "weight: [<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([6., 6., 6.], dtype=float32)>]\n",
            "non-trainable weight: [<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([6., 6., 6.], dtype=float32)>]\n",
            "trainable weight: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-gmvQNUqrzI"
      },
      "source": [
        "当定义网络时不知道网络的维度是可以重写build()函数，用获得的shape构建网络"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIPT0sRDqrDJ",
        "outputId": "2a3f1254-f721-4886-f3f1-18e6e8bd6dd7"
      },
      "source": [
        "\n",
        "class MyLayer(layers.Layer):\n",
        "    def __init__(self, unit=32):\n",
        "        super(MyLayer, self).__init__()\n",
        "        self.unit = unit\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        # 在build时获取input_shape\n",
        "        self.weight = self.add_weight(shape=(input_shape[-1], self.unit),\n",
        "                                     initializer=keras.initializers.RandomNormal(),\n",
        "                                     trainable=True)\n",
        "        self.bias = self.add_weight(shape=(self.unit,),\n",
        "                                   initializer=keras.initializers.Zeros(),\n",
        "                                   trainable=True)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.weight) + self.bias\n",
        "\n",
        "my_layer = MyLayer(3)\n",
        "x = tf.ones((3,5))\n",
        "out = my_layer(x)\n",
        "print(out)\n",
        "\n",
        "my_layer = MyLayer(3)\n",
        "\n",
        "x = tf.ones((2,2))\n",
        "out = my_layer(x)\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[-0.0727546   0.0825604   0.04932299]\n",
            " [-0.0727546   0.0825604   0.04932299]\n",
            " [-0.0727546   0.0825604   0.04932299]], shape=(3, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-0.05158995  0.09036048 -0.02937933]\n",
            " [-0.05158995  0.09036048 -0.02937933]], shape=(2, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt7L8mEPrrH_"
      },
      "source": [
        "使用子层递归构建网络层\n",
        "\n",
        "\n",
        "---\n",
        "可以在自定义网络层中调用其他自定义网络层\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faUZG6PLr6FN",
        "outputId": "208efdbc-73f6-4dea-81ba-d55d2632c61d"
      },
      "source": [
        "\n",
        "class MyBlock(layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(MyBlock, self).__init__()\n",
        "        # 其他自定义网络层\n",
        "        self.layer1 = MyLayer(32)\n",
        "        self.layer2 = MyLayer(16)\n",
        "        self.layer3 = MyLayer(2)\n",
        "    def call(self, inputs):\n",
        "        h1 = self.layer1(inputs)\n",
        "        h1 = tf.nn.relu(h1)\n",
        "        h2 = self.layer2(h1)\n",
        "        h2 = tf.nn.relu(h2)\n",
        "        return self.layer3(h2)\n",
        "\n",
        "my_block = MyBlock()\n",
        "print('trainable weights:', len(my_block.trainable_weights))\n",
        "y = my_block(tf.ones(shape=(3, 64)))\n",
        "# 构建网络在build()里面，所以执行了才有网络\n",
        "print('trainable weights:', len(my_block.trainable_weights))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trainable weights: 0\n",
            "trainable weights: 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5UvOWAolr5E"
      },
      "source": [
        "\n",
        "可以通过构建网络层的方法来收集loss，并可以递归调用。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TU7VMLw9lnj5",
        "outputId": "dd6167f7-6aa6-4591-a714-10feaf487aed"
      },
      "source": [
        "class LossLayer(layers.Layer):\n",
        "  \n",
        "  def __init__(self, rate=1e-2):\n",
        "    super(LossLayer, self).__init__()\n",
        "    self.rate = rate\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    # 添加loss\n",
        "    self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
        "    return inputs\n",
        "\n",
        "class OutLayer(layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(OutLayer, self).__init__()\n",
        "        self.loss_fun=LossLayer(1e-2)\n",
        "    def call(self, inputs):\n",
        "        # 就一个loss层\n",
        "        return self.loss_fun(inputs)\n",
        "    \n",
        "my_layer = OutLayer()\n",
        "print(len(my_layer.losses)) # 还未call\n",
        "y = my_layer(tf.zeros(1,1))\n",
        "print(len(my_layer.losses)) # 执行call之后\n",
        "y = my_layer(tf.zeros(1,1))\n",
        "print(len(my_layer.losses)) # call之前会重新置0"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k29gLwqylwuI"
      },
      "source": [
        "如果中间调用了keras网络层，里面的正则化loss也会被加入进来"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y2Y7Mi8lu6C",
        "outputId": "cee6b829-164e-4220-8faf-bd4a3fbb77eb"
      },
      "source": [
        "class OuterLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(OuterLayer, self).__init__()\n",
        "        # 子层中正则化loss也会添加到总的loss中\n",
        "        self.dense = layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        return self.dense(inputs)\n",
        "\n",
        "\n",
        "my_layer = OuterLayer()\n",
        "y = my_layer(tf.zeros((1,1)))\n",
        "print(my_layer.losses) \n",
        "print(my_layer.weights)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor: shape=(), dtype=float32, numpy=0.001676522>]\n",
            "[<tf.Variable 'outer_layer_2/dense_2/kernel:0' shape=(1, 32) dtype=float32, numpy=\n",
            "array([[-0.06407502, -0.10402119,  0.22935128, -0.13034222, -0.4056217 ,\n",
            "         0.13479519,  0.02153456,  0.19563931, -0.23100692,  0.31607872,\n",
            "         0.30772513,  0.29738307, -0.0422852 ,  0.04634547, -0.1645967 ,\n",
            "         0.15944034,  0.41192138, -0.32810402, -0.20457551,  0.2781303 ,\n",
            "         0.24716926, -0.09292975,  0.09127665, -0.06151396, -0.17136294,\n",
            "        -0.19148083, -0.0719462 ,  0.00594112, -0.01029357,  0.30333388,\n",
            "        -0.42333055, -0.39593688]], dtype=float32)>, <tf.Variable 'outer_layer_2/dense_2/bias:0' shape=(32,) dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "      dtype=float32)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNnR0BZil1vy"
      },
      "source": [
        "其他网络层配置\n",
        "\n",
        "\n",
        "---\n",
        "使自己的网络层可以序列化\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyJSDSbUl2yY",
        "outputId": "80874fcf-6e71-47db-dfb7-a5c33b400966"
      },
      "source": [
        "class Linear(layers.Layer):\n",
        "\n",
        "    def __init__(self, units=32, **kwargs):\n",
        "        super(Linear, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)\n",
        "        self.b = self.add_weight(shape=(self.units,),\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n",
        "    \n",
        "    def get_config(self):\n",
        "        # 获取网络配置，用于实现序列化\n",
        "        config = super(Linear, self).get_config()\n",
        "        config.update({'units':self.units})\n",
        "        return config\n",
        "    \n",
        "    \n",
        "layer = Linear(125)\n",
        "config = layer.get_config()\n",
        "print(config)\n",
        "# 从配置中构建网络，（已知网络结构，不知超参的情况）\n",
        "new_layer = Linear.from_config(config)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'name': 'linear_1', 'trainable': True, 'dtype': 'float32', 'units': 125}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKQrKcwLl8_I"
      },
      "source": [
        "如果在反序列化中(从配置中构建网络)需要更大的灵活性，可以重写from_config方法。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCigl33ImN6z"
      },
      "source": [
        "def from_config(cls, config):\n",
        "    return cls(**config)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgbVf1j2mO7f"
      },
      "source": [
        "3.2 配置训练时特有参数\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "有一些网络层， 如BatchNormalization层和Dropout层，在训练和推理中具有不同的行为，对于此类层，则需要在方法中使用train等参数进行控制。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLjugm6BmUye"
      },
      "source": [
        "class MyDropout(layers.Layer):\n",
        "    def __init__(self, rate, **kwargs):\n",
        "        super(MyDropout, self).__init__(**kwargs)\n",
        "        self.rate = rate\n",
        "    def call(self, inputs, training=None):\n",
        "        return tf.cond(training, \n",
        "                       lambda: tf.nn.dropout(inputs, rate=self.rate),\n",
        "                      lambda: inputs)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8U245kjmXIV"
      },
      "source": [
        "\n",
        "4 构建自己的模型\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "通常，我们使用Layer类来定义内部计算块，并使用Model类来定义外部模型 - 即要训练的对象。\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Model类与Layer的区别：\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "它对外开放内置的训练，评估和预测函数（model.fit(),model.evaluate(),model.predict()）。\n",
        "它通过model.layers属性开放其内部网络层列表。\n",
        "它对外开放保存和序列化API。\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "4.1 自定义模型\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "下面通过构建一个变分自编码器（VAE），来介绍如何构建自己的网络， 并使用内置的函数进行训练。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YufO6vQMmhhU"
      },
      "source": [
        "# 采样网络\n",
        "class Sampling(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5*z_log_var) * epsilon\n",
        "\n",
        "# 编码器\n",
        "class Encoder(layers.Layer):\n",
        "    def __init__(self, latent_dim=32, \n",
        "                intermediate_dim=64, name='encoder', **kwargs):\n",
        "        super(Encoder, self).__init__(name=name, **kwargs)\n",
        "        self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\n",
        "        self.dense_mean = layers.Dense(latent_dim)\n",
        "        self.dense_log_var = layers.Dense(latent_dim)\n",
        "        self.sampling = Sampling()\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        h1 = self.dense_proj(inputs)\n",
        "        # 获取z_mean和z_log_var\n",
        "        z_mean = self.dense_mean(h1)\n",
        "        z_log_var = self.dense_log_var(h1)\n",
        "        # 进行采样\n",
        "        z = self.sampling((z_mean, z_log_var))\n",
        "        return z_mean, z_log_var, z\n",
        "\n",
        "# 解码器\n",
        "class Decoder(layers.Layer):\n",
        "    def __init__(self, original_dim, \n",
        "                 intermediate_dim=64, name='decoder', **kwargs):\n",
        "        super(Decoder, self).__init__(name=name, **kwargs)\n",
        "        self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\n",
        "        self.dense_output = layers.Dense(original_dim, activation='sigmoid')\n",
        "    def call(self, inputs):\n",
        "        # 两层全连接网络\n",
        "        h1 = self.dense_proj(inputs)\n",
        "        return self.dense_output(h1)\n",
        "\n",
        "\n",
        "# 变分自编码器\n",
        "class VAE(tf.keras.Model):\n",
        "    def __init__(self, original_dim, latent_dim=32, \n",
        "                intermediate_dim=64, name='encoder', **kwargs):\n",
        "        super(VAE, self).__init__(name=name, **kwargs)\n",
        "    \n",
        "        self.original_dim = original_dim\n",
        "        self.encoder = Encoder(latent_dim=latent_dim,\n",
        "                              intermediate_dim=intermediate_dim)\n",
        "        self.decoder = Decoder(original_dim=original_dim,\n",
        "                              intermediate_dim=intermediate_dim)\n",
        "    def call(self, inputs):\n",
        "        # 编码\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        # 解码\n",
        "        reconstructed = self.decoder(z)\n",
        "        # 获取损失函数\n",
        "        kl_loss = -0.5*tf.reduce_sum(\n",
        "            z_log_var-tf.square(z_mean)-tf.exp(z_log_var)+1)\n",
        "        self.add_loss(kl_loss)\n",
        "        return reconstructed"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm28H52KtdD3"
      },
      "source": [
        "训练VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmS5rPxFteEK",
        "outputId": "31cb5318-aa0c-4f33-894d-aaa1e12cee9e"
      },
      "source": [
        "\n",
        "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
        "vae = VAE(784,32,64)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
        "vae.fit(x_train, x_train, epochs=3, batch_size=64)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/3\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 1.0417\n",
            "Epoch 2/3\n",
            "938/938 [==============================] - 3s 4ms/step - loss: 0.0691\n",
            "Epoch 3/3\n",
            "938/938 [==============================] - 4s 4ms/step - loss: 0.0679\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc1861c6b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjOcfFa5uxPE"
      },
      "source": [
        "自己编写训练方法"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXcv6J3quyVg",
        "outputId": "bfd4aac5-a8f2-4e58-c6b5-f9f92c8767a7"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "original_dim = 784\n",
        "vae = VAE(original_dim, 64, 32)\n",
        "\n",
        "# 优化器\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "# 损失函数\n",
        "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "# 评价指标\n",
        "loss_metric = tf.keras.metrics.Mean()\n",
        "\n",
        "# 训练循环\n",
        "for epoch in range(3):\n",
        "    print('Start of epoch %d' % (epoch,))\n",
        "\n",
        "    # 每批次训练\n",
        "    for step, x_batch_train in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # 前向传播\n",
        "            reconstructed = vae(x_batch_train)\n",
        "            # 计算loss\n",
        "            loss = mse_loss_fn(x_batch_train, reconstructed)\n",
        "            loss += sum(vae.losses)  # Add KLD regularization loss\n",
        "        # 计算梯度\n",
        "        grads = tape.gradient(loss, vae.trainable_variables)\n",
        "        # 反向传播\n",
        "        optimizer.apply_gradients(zip(grads, vae.trainable_variables))\n",
        "        # 统计指标\n",
        "        loss_metric(loss)\n",
        "        # 输出\n",
        "        if step % 100 == 0:\n",
        "            print('step %s: mean loss = %s' % (step, loss_metric.result()))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 0\n",
            "step 0: mean loss = tf.Tensor(291.07437, shape=(), dtype=float32)\n",
            "step 100: mean loss = tf.Tensor(8.775421, shape=(), dtype=float32)\n",
            "step 200: mean loss = tf.Tensor(4.4646726, shape=(), dtype=float32)\n",
            "step 300: mean loss = tf.Tensor(3.0108864, shape=(), dtype=float32)\n",
            "step 400: mean loss = tf.Tensor(2.280789, shape=(), dtype=float32)\n",
            "step 500: mean loss = tf.Tensor(1.8414719, shape=(), dtype=float32)\n",
            "step 600: mean loss = tf.Tensor(1.5473853, shape=(), dtype=float32)\n",
            "step 700: mean loss = tf.Tensor(1.3372066, shape=(), dtype=float32)\n",
            "step 800: mean loss = tf.Tensor(1.1792815, shape=(), dtype=float32)\n",
            "step 900: mean loss = tf.Tensor(1.0561988, shape=(), dtype=float32)\n",
            "Start of epoch 1\n",
            "step 0: mean loss = tf.Tensor(1.0162915, shape=(), dtype=float32)\n",
            "step 100: mean loss = tf.Tensor(0.92539376, shape=(), dtype=float32)\n",
            "step 200: mean loss = tf.Tensor(0.85041577, shape=(), dtype=float32)\n",
            "step 300: mean loss = tf.Tensor(0.7874146, shape=(), dtype=float32)\n",
            "step 400: mean loss = tf.Tensor(0.7338833, shape=(), dtype=float32)\n",
            "step 500: mean loss = tf.Tensor(0.687626, shape=(), dtype=float32)\n",
            "step 600: mean loss = tf.Tensor(0.6473912, shape=(), dtype=float32)\n",
            "step 700: mean loss = tf.Tensor(0.6120533, shape=(), dtype=float32)\n",
            "step 800: mean loss = tf.Tensor(0.58079445, shape=(), dtype=float32)\n",
            "step 900: mean loss = tf.Tensor(0.55286425, shape=(), dtype=float32)\n",
            "Start of epoch 2\n",
            "step 0: mean loss = tf.Tensor(0.5430513, shape=(), dtype=float32)\n",
            "step 100: mean loss = tf.Tensor(0.51906854, shape=(), dtype=float32)\n",
            "step 200: mean loss = tf.Tensor(0.49737874, shape=(), dtype=float32)\n",
            "step 300: mean loss = tf.Tensor(0.4776479, shape=(), dtype=float32)\n",
            "step 400: mean loss = tf.Tensor(0.45971775, shape=(), dtype=float32)\n",
            "step 500: mean loss = tf.Tensor(0.4431989, shape=(), dtype=float32)\n",
            "step 600: mean loss = tf.Tensor(0.4280483, shape=(), dtype=float32)\n",
            "step 700: mean loss = tf.Tensor(0.41404963, shape=(), dtype=float32)\n",
            "step 800: mean loss = tf.Tensor(0.40112337, shape=(), dtype=float32)\n",
            "step 900: mean loss = tf.Tensor(0.38907778, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}