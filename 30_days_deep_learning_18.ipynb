{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "30_days_deep_learning_18.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOa3bKiUfFq9bigiq655eHL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Allen123321/DEMO-DL/blob/master/30_days_deep_learning_18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhrTbSmvAJNw"
      },
      "source": [
        "## 5-7,优化器optimizers\r\n",
        "机器学习界有一群炼丹师，他们每天的日常是：\r\n",
        "\r\n",
        "拿来药材（数据），架起八卦炉（模型），点着六味真火（优化算法），就摇着蒲扇等着丹药出炉了。\r\n",
        "\r\n",
        "不过，当过厨子的都知道，同样的食材，同样的菜谱，但火候不一样了，这出来的口味可是千差万别。火小了夹生，火大了易糊，火不匀则半生半糊。\r\n",
        "\r\n",
        "机器学习也是一样，模型优化算法的选择直接关系到最终模型的性能。有时候效果不好，未必是特征的问题或者模型设计的问题，很可能就是优化算法的问题。\r\n",
        "\r\n",
        "深度学习优化算法大概经历了 SGD -> SGDM -> NAG ->Adagrad -> Adadelta(RMSprop) -> Adam -> Nadam 这样的发展历程。\r\n",
        "\r\n",
        "[一个框架看懂优化算法之异同 SGD/AdaGrad/Adam](https://zhuanlan.zhihu.com/p/32230623)\r\n",
        "\r\n",
        "一些爱写论文的炼丹师由于追求评估指标效果，可能会偏爱前期使用Adam优化器快速下降，后期使用SGD并精调优化器参数得到更好的结果。\r\n",
        "\r\n",
        "此外目前也有一些前沿的优化算法，据称效果比Adam更好，例如LazyAdam, Look-ahead, RAdam, Ranger等."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soQlQ5vPDqAN"
      },
      "source": [
        "一，优化器的使用\r\n",
        "优化器主要使用apply_gradients方法传入变量和对应梯度从而来对给定变量进行迭代，或者直接使用minimize方法对目标函数进行迭代优化。\r\n",
        "\r\n",
        "当然，更常见的使用是在编译时将优化器传入keras的Model,通过调用model.fit实现对Loss的的迭代优化。\r\n",
        "\r\n",
        "初始化优化器时会创建一个变量optimier.iterations用于记录迭代的次数。因此优化器和tf.Variable一样，一般需要在@tf.function外创建。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K7RPj1IAFF7"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np \r\n",
        "\r\n",
        "#打印时间分割线\r\n",
        "@tf.function\r\n",
        "def printbar():\r\n",
        "    ts = tf.timestamp()\r\n",
        "    today_ts = ts%(24*60*60)\r\n",
        "\r\n",
        "    hour = tf.cast(today_ts//3600+2,tf.int32)%tf.constant(24)\r\n",
        "    minite = tf.cast((today_ts%3600)//60,tf.int32)\r\n",
        "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\r\n",
        "    \r\n",
        "    def timeformat(m):\r\n",
        "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\r\n",
        "            return(tf.strings.format(\"0{}\",m))\r\n",
        "        else:\r\n",
        "            return(tf.strings.format(\"{}\",m))\r\n",
        "    \r\n",
        "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\r\n",
        "                timeformat(second)],separator = \":\")\r\n",
        "    tf.print(\"==========\"*8,end = \"\")\r\n",
        "    tf.print(timestring)\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvPtRDWyEDLO",
        "outputId": "61d535ca-5ded-410d-9373-5444296187ea"
      },
      "source": [
        "# 求f(x) = a*x**2 + b*x + c的最小值\r\n",
        "\r\n",
        "# 使用optimizer.apply_gradients\r\n",
        "\r\n",
        "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\r\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def minimizef():\r\n",
        "  a = tf.constant(1.0)\r\n",
        "  b = tf.constant(-2.0)\r\n",
        "  c = tf.constant(1.0)\r\n",
        "\r\n",
        "  while tf.constant(True):\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "      y = a*tf.pow(x,2) + b*x + c\r\n",
        "    dy_dx = tape.gradient(y,x)\r\n",
        "    optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])\r\n",
        "    #迭代终止条件\r\n",
        "    if tf.abs(dy_dx)<tf.constant(0.00001):\r\n",
        "      break\r\n",
        "\r\n",
        "    if tf.math.mod(optimizer.iterations,100)==0:\r\n",
        "      printbar()\r\n",
        "      tf.print(\"step = \",optimizer.iterations)\r\n",
        "      tf.print(\"x = \", x)\r\n",
        "      tf.print(\"\")\r\n",
        "\r\n",
        "  y = a*tf.pow(x,2) + b*x + c\r\n",
        "\r\n",
        "  return y\r\n",
        "\r\n",
        "\r\n",
        "tf.print(\"y =\",minimizef())\r\n",
        "tf.print(\"x =\",x)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================11:01:20\n",
            "step =  100\n",
            "x =  0.867380381\n",
            "\n",
            "================================================================================11:01:20\n",
            "step =  200\n",
            "x =  0.98241204\n",
            "\n",
            "================================================================================11:01:20\n",
            "step =  300\n",
            "x =  0.997667611\n",
            "\n",
            "================================================================================11:01:20\n",
            "step =  400\n",
            "x =  0.999690652\n",
            "\n",
            "================================================================================11:01:20\n",
            "step =  500\n",
            "x =  0.999959\n",
            "\n",
            "================================================================================11:01:20\n",
            "step =  600\n",
            "x =  0.999994457\n",
            "\n",
            "y = 0\n",
            "x = 0.999995172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PthTRELqFTjd",
        "outputId": "6d7945c6-8b11-4d82-f6c3-d0ae56d6153c"
      },
      "source": [
        "# 求f(x) = a*x**2 + b*x + c的最小值\r\n",
        "\r\n",
        "# 使用optimizer.minimize\r\n",
        "\r\n",
        "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\r\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)   \r\n",
        "\r\n",
        "def f():   \r\n",
        "    a = tf.constant(1.0)\r\n",
        "    b = tf.constant(-2.0)\r\n",
        "    c = tf.constant(1.0)\r\n",
        "    y = a*tf.pow(x,2)+b*x+c\r\n",
        "    return(y)\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def train(epoch = 1000):  \r\n",
        "    for i in tf.range(epoch):\r\n",
        "      optimizer.minimize(f,[x])\r\n",
        "      \r\n",
        "    tf.print(\"epoch = \",optimizer.iterations)\r\n",
        "    return(f())\r\n",
        "\r\n",
        "train(1000)\r\n",
        "tf.print(\"y = \",f())\r\n",
        "tf.print(\"x = \",x)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch =  1000\n",
            "y =  0\n",
            "x =  0.99999851\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4CI_CqkIetG",
        "outputId": "6d3644e1-2523-4830-c1e0-e050c5f1e960"
      },
      "source": [
        "# 求f(x) = a*x**2 + b*x + c的最小值\r\n",
        "# 使用model.fit\r\n",
        "\r\n",
        "tf.keras.backend.clear_session()\r\n",
        "\r\n",
        "class FakeModel(tf.keras.models.Model):\r\n",
        "    def __init__(self,a,b,c):\r\n",
        "        super(FakeModel,self).__init__()\r\n",
        "        self.a = a\r\n",
        "        self.b = b\r\n",
        "        self.c = c\r\n",
        "    \r\n",
        "    def build(self):\r\n",
        "        self.x = tf.Variable(0.0,name = \"x\")\r\n",
        "        self.built = True\r\n",
        "    \r\n",
        "    def call(self,features):\r\n",
        "        loss  = self.a*(self.x)**2+self.b*(self.x)+self.c\r\n",
        "        return(tf.ones_like(features)*loss)\r\n",
        "    \r\n",
        "def myloss(y_true,y_pred):\r\n",
        "    return tf.reduce_mean(y_pred)\r\n",
        "\r\n",
        "model = FakeModel(tf.constant(1.0),tf.constant(-2.0),tf.constant(1.0))\r\n",
        "\r\n",
        "model.build()\r\n",
        "model.summary()\r\n",
        "\r\n",
        "model.compile(optimizer = \r\n",
        "              tf.keras.optimizers.SGD(learning_rate=0.01),loss = myloss)\r\n",
        "history = model.fit(tf.zeros((100,2)),\r\n",
        "                    tf.ones(100),batch_size = 1,epochs = 10)  #迭代1000次"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"fake_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Total params: 1\n",
            "Trainable params: 1\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 0s 729us/step - loss: 0.4930\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 0s 790us/step - loss: 0.0087\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 0s 784us/step - loss: 1.5250e-04\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 0s 938us/step - loss: 2.6838e-06\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 0s 722us/step - loss: 4.3807e-08\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 0s 721us/step - loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 0s 718us/step - loss: 0.0000e+00\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 0s 741us/step - loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 0s 764us/step - loss: 0.0000e+00\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 0s 670us/step - loss: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "om3Y9g_6JyHt",
        "outputId": "ea7b7654-5057-478a-8a32-e39e6801e9f7"
      },
      "source": [
        "tf.print(\"x=\",model.x)\r\n",
        "tf.print(\"loss=\",model(tf.constant(0.0)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x= 0.99999851\n",
            "loss= 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GEiXEFIJ7tX"
      },
      "source": [
        "## 二，内置优化器\r\n",
        "深度学习优化算法大概经历了 SGD -> SGDM -> NAG ->Adagrad -> Adadelta(RMSprop) -> Adam -> Nadam 这样的发展历程。\r\n",
        "\r\n",
        "在keras.optimizers子模块中，它们基本上都有对应的类的实现。\r\n",
        "\r\n",
        "SGD, 默认参数为纯SGD, 设置momentum参数不为0实际上变成SGDM, 考虑了一阶动量, 设置 nesterov为True后变成NAG，即 Nesterov Accelerated Gradient，在计算梯度时计算的是向前走一步所在位置的梯度。\r\n",
        "\r\n",
        "Adagrad, 考虑了二阶动量，对于不同的参数有不同的学习率，即自适应学习率。缺点是学习率单调下降，可能后期学习速率过慢乃至提前停止学习。\r\n",
        "\r\n",
        "RMSprop, 考虑了二阶动量，对于不同的参数有不同的学习率，即自适应学习率，对Adagrad进行了优化，通过指数平滑只考虑一定窗口内的二阶动量。\r\n",
        "\r\n",
        "Adadelta, 考虑了二阶动量，与RMSprop类似，但是更加复杂一些，自适应性更强。\r\n",
        "\r\n",
        "Adam, 同时考虑了一阶动量和二阶动量，可以看成RMSprop上进一步考虑了一阶动量。\r\n",
        "\r\n",
        "Nadam, 在Adam基础上进一步考虑了 Nesterov Acceleration。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu3axdxZKbTt"
      },
      "source": [
        "## 5-8,回调函数callbacks\r\n",
        "\r\n",
        "tf.keras的回调函数实际上是一个类，一般是在model.fit时作为参数指定，用于控制在训练过程开始或者在训练过程结束，在每个epoch训练开始或者训练结束，在每个batch训练开始或者训练结束时执行一些操作，例如收集一些日志信息，改变学习率等超参数，提前终止训练过程等等。\r\n",
        "\r\n",
        "同样地，针对model.evaluate或者model.predict也可以指定callbacks参数，用于控制在评估或预测开始或者结束时，在每个batch开始或者结束时执行一些操作，但这种用法相对少见。\r\n",
        "\r\n",
        "大部分时候，keras.callbacks子模块中定义的回调函数类已经足够使用了，如果有特定的需要，我们也可以通过对keras.callbacks.Callbacks实施子类化构造自定义的回调函数。\r\n",
        "\r\n",
        "所有回调函数都继承至 keras.callbacks.Callbacks基类，拥有params和model这两个属性。\r\n",
        "\r\n",
        "其中params 是一个dict，记录了训练相关参数 (例如 verbosity, batch size, number of epochs 等等)。\r\n",
        "\r\n",
        "model即当前关联的模型的引用。\r\n",
        "\r\n",
        "此外，对于回调类中的一些方法如on_epoch_begin,on_batch_end，还会有一个输入参数logs, 提供有关当前epoch或者batch的一些信息，并能够记录计算结果，如果model.fit指定了多个回调函数类，这些logs变量将在这些回调函数类的同名函数间依顺序传递。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Uiv6Q3wNHUG"
      },
      "source": [
        "## 一，内置回调函数\r\n",
        "\r\n",
        "BaseLogger： 收集每个epoch上metrics在各个batch上的平均值，对stateful_metrics参数中的带中间状态的指标直接拿最终值无需对各个batch平均，指标均值结果将添加到logs变量中。该回调函数被所有模型默认添加，且是第一个被添加的。\r\n",
        "\r\n",
        "History： 将BaseLogger计算的各个epoch的metrics结果记录到history这个dict变量中，并作为model.fit的返回值。该回调函数被所有模型默认添加，在BaseLogger之后被添加。\r\n",
        "\r\n",
        "EarlyStopping： 当被监控指标在设定的若干个epoch后没有提升，则提前终止训练。\r\n",
        "\r\n",
        "TensorBoard： 为Tensorboard可视化保存日志信息。支持评估指标，计算图，模型参数等的可视化。\r\n",
        "\r\n",
        "ModelCheckpoint： 在每个epoch后保存模型。\r\n",
        "\r\n",
        "ReduceLROnPlateau：如果监控指标在设定的若干个epoch后没有提升，则以一定的因子减少学习率。\r\n",
        "\r\n",
        "TerminateOnNaN：如果遇到loss为NaN，提前终止训练。\r\n",
        "\r\n",
        "LearningRateScheduler：学习率控制器。给定学习率lr和epoch的函数关系，根据该函数关系在每个epoch前调整学习率。\r\n",
        "\r\n",
        "CSVLogger：将每个epoch后的logs结果记录到CSV文件中。\r\n",
        "\r\n",
        "ProgbarLogger：将每个epoch后的logs结果打印到标准输出流中。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtX6TOw7NbwE"
      },
      "source": [
        "## 二，自定义回调函数\r\n",
        "可以使用callbacks.LambdaCallback编写较为简单的回调函数，也可以通过对callbacks.Callback子类化编写更加复杂的回调函数逻辑。\r\n",
        "\r\n",
        "如果需要深入学习tf.Keras中的回调函数，不要犹豫阅读内置回调函数的源代码。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9BmtLgLNfQf"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import layers,models,losses,metrics,callbacks\r\n",
        "import tensorflow.keras.backend as K "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbYFk4mzNpCG",
        "outputId": "3bcb47ae-05c5-4d9a-a6fc-48c8b553f00a"
      },
      "source": [
        "!wget https://github.com/lyhue1991/eat_tensorflow2_in_30_days/blob/master/data/keras_log.json"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-07 09:39:45--  https://github.com/lyhue1991/eat_tensorflow2_in_30_days/blob/master/data/keras_log.json\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘keras_log.json’\n",
            "\n",
            "keras_log.json          [ <=>                ]  86.07K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-01-07 09:39:45 (5.23 MB/s) - ‘keras_log.json’ saved [88132]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvmjQMEnN3Ai"
      },
      "source": [
        "# 示范使用LambdaCallback编写较为简单的回调函数\r\n",
        "\r\n",
        "import json\r\n",
        "json_log = open('keras_log.json', mode='wt', buffering=1)\r\n",
        "json_logging_callback = callbacks.LambdaCallback(\r\n",
        "    on_epoch_end=lambda epoch, logs: json_log.write(\r\n",
        "        json.dumps(dict(epoch = epoch,**logs)) + '\\n'),\r\n",
        "    on_train_end=lambda logs: json_log.close()\r\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blUDuEX5OJXj"
      },
      "source": [
        "# 示范通过Callback子类化编写回调函数（LearningRateScheduler的源代码）\r\n",
        "\r\n",
        "class LearningRateScheduler(callbacks.Callback):\r\n",
        "    \r\n",
        "    def __init__(self, schedule, verbose=0):\r\n",
        "        super(LearningRateScheduler, self).__init__()\r\n",
        "        self.schedule = schedule\r\n",
        "        self.verbose = verbose\r\n",
        "\r\n",
        "    def on_epoch_begin(self, epoch, logs=None):\r\n",
        "        if not hasattr(self.model.optimizer, 'lr'):\r\n",
        "            raise ValueError('Optimizer must have a \"lr\" attribute.')\r\n",
        "        try:  \r\n",
        "            lr = float(K.get_value(self.model.optimizer.lr))\r\n",
        "            lr = self.schedule(epoch, lr)\r\n",
        "        except TypeError:  # Support for old API for backward compatibility\r\n",
        "            lr = self.schedule(epoch)\r\n",
        "        if not isinstance(lr, (tf.Tensor, float, np.float32, np.float64)):\r\n",
        "            raise ValueError('The output of the \"schedule\" function '\r\n",
        "                             'should be float.')\r\n",
        "        if isinstance(lr, ops.Tensor) and not lr.dtype.is_floating:\r\n",
        "            raise ValueError('The dtype of Tensor should be float')\r\n",
        "        K.set_value(self.model.optimizer.lr, K.get_value(lr))\r\n",
        "        if self.verbose > 0:\r\n",
        "            print('\\nEpoch %05d: LearningRateScheduler reducing learning '\r\n",
        "                 'rate to %s.' % (epoch + 1, lr))\r\n",
        "\r\n",
        "    def on_epoch_end(self, epoch, logs=None):\r\n",
        "        logs = logs or {}\r\n",
        "        logs['lr'] = K.get_value(self.model.optimizer.lr)"
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}