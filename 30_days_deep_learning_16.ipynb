{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "30_days_deep_learning_16.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOOCEm9Q02XFVYlLmc5kSar",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Allen123321/DEMO-DL/blob/master/30_days_deep_learning_16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y43Mrzn__74N"
      },
      "source": [
        "## 5-3,激活函数activation\r\n",
        "激活函数在深度学习中扮演着非常重要的角色，它给网络赋予了非线性，从而使得神经网络能够拟合任意复杂的函数。\r\n",
        "\r\n",
        "如果没有激活函数，无论多复杂的网络，都等价于单一的线性变换，无法对非线性函数进行拟合。\r\n",
        "\r\n",
        "目前，深度学习中最流行的激活函数为 relu, 但也有些新推出的激活函数，例如 swish、GELU 据称效果优于relu激活函数。\r\n",
        "\r\n",
        "激活函数的综述介绍可以参考下面两篇文章。\r\n",
        "\r\n",
        "[一文概览深度学习中的激活函数](https://zhuanlan.zhihu.com/p/98472075)\r\n",
        "\r\n",
        "[从ReLU到GELU，一文概览神经网络的激活函数](https://zhuanlan.zhihu.com/p/98863801)\r\n",
        "\r\n",
        "## 一，常用激活函数\r\n",
        "tf.nn.sigmoid：将实数压缩到0到1之间，一般只在二分类的最后输出层使用。主要缺陷为存在梯度消失问题，计算复杂度高，输出不以0为中心。\r\n",
        "\r\n",
        "tf.nn.softmax：sigmoid的多分类扩展，一般只在多分类问题的最后输出层使用。\r\n",
        "\r\n",
        "tf.nn.tanh：将实数压缩到-1到1之间，输出期望为0。主要缺陷为存在梯度消失问题，计算复杂度高。\r\n",
        "\r\n",
        "tf.nn.relu：修正线性单元，最流行的激活函数。一般隐藏层使用。主要缺陷是：输出不以0为中心，输入小于0时存在梯度消失问题(死亡relu)。\r\n",
        "\r\n",
        "tf.nn.leaky_relu：对修正线性单元的改进，解决了死亡relu问题。\r\n",
        "\r\n",
        "tf.nn.elu：指数线性单元。对relu的改进，能够缓解死亡relu问题。\r\n",
        "\r\n",
        "tf.nn.selu：扩展型指数线性单元。在权重用tf.keras.initializers.lecun_normal初始化前提下能够对神经网络进行自归一化。不可能出现梯度爆炸或者梯度消失问题。需要和Dropout的变种AlphaDropout一起使用。\r\n",
        "\r\n",
        "tf.nn.swish：自门控激活函数。谷歌出品，相关研究指出用swish替代relu将获得轻微效果提升。\r\n",
        "\r\n",
        "gelu：高斯误差线性单元激活函数。在Transformer中表现最好。tf.nn模块尚没有实现该函数。\r\n",
        "\r\n",
        "## 二，在模型中使用激活函数\r\n",
        "在keras模型中使用激活函数一般有两种方式，一种是作为某些层的activation参数指定，另一种是显式添加layers.Activation激活层。\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RC_qjmjX_1yj",
        "outputId": "7ddf4ed6-7d62-41e8-8988-a0bab04f467f"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import layers,models\r\n",
        "\r\n",
        "model = models.Sequential()\r\n",
        "model.add(layers.Dense(32,input_shape = (None,16),activation = tf.nn.relu)) #通过activation参数指定\r\n",
        "model.add(layers.Dense(10))\r\n",
        "model.add(layers.Activation(tf.nn.softmax)) # 显式添加layers.Activation激活层\r\n",
        "model.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, None, 32)          544       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, None, 10)          330       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, None, 10)          0         \n",
            "=================================================================\n",
            "Total params: 874\n",
            "Trainable params: 874\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf8APC8ELTf8"
      },
      "source": [
        "## 5-4,模型层layers\r\n",
        "深度学习模型一般由各种模型层组合而成。\r\n",
        "\r\n",
        "tf.keras.layers内置了非常丰富的各种功能的模型层。例如，\r\n",
        "\r\n",
        "layers.Dense,layers.Flatten,layers.Input,layers.DenseFeature,layers.Dropout\r\n",
        "\r\n",
        "layers.Conv2D,layers.MaxPooling2D,layers.Conv1D\r\n",
        "\r\n",
        "layers.Embedding,layers.GRU,layers.LSTM,layers.Bidirectional等等。\r\n",
        "\r\n",
        "如果这些内置模型层不能够满足需求，我们也可以通过编写tf.keras.Lambda匿名模型层或继承tf.keras.layers.Layer基类构建自定义的模型层。\r\n",
        "\r\n",
        "其中tf.keras.Lambda匿名模型层只适用于构造没有学习参数的模型层。\r\n",
        "## 一，内置模型层\r\n",
        "一些常用的内置模型层简单介绍如下。\r\n",
        "\r\n",
        "### 基础层\r\n",
        "\r\n",
        "Dense：密集连接层。参数个数 = 输入层特征数× 输出层特征数(weight)＋ 输出层特征数(bias)\r\n",
        "\r\n",
        "Activation：激活函数层。一般放在Dense层后面，等价于在Dense层中指定activation。\r\n",
        "\r\n",
        "Dropout：随机置零层。训练期间以一定几率将输入置0，一种正则化手段。\r\n",
        "\r\n",
        "BatchNormalization：批标准化层。通过线性变换将输入批次缩放平移到稳定的均值和标准差。可以增强模型对输入不同分布的适应性，加快模型训练速度，有轻微正则化效果。一般在激活函数之前使用。\r\n",
        "\r\n",
        "SpatialDropout2D：空间随机置零层。训练期间以一定几率将整个特征图置0，一种正则化手段，有利于避免特征图之间过高的相关性。\r\n",
        "\r\n",
        "Input：输入层。通常使用Functional API方式构建模型时作为第一层。\r\n",
        "\r\n",
        "DenseFeature：特征列接入层，用于接收一个特征列列表并产生一个密集连接层。\r\n",
        "\r\n",
        "Flatten：压平层，用于将多维张量压成一维。\r\n",
        "\r\n",
        "Reshape：形状重塑层，改变输入张量的形状。\r\n",
        "\r\n",
        "Concatenate：拼接层，将多个张量在某个维度上拼接。\r\n",
        "\r\n",
        "Add：加法层。\r\n",
        "\r\n",
        "Subtract： 减法层。\r\n",
        "\r\n",
        "Maximum：取最大值层。\r\n",
        "\r\n",
        "Minimum：取最小值层。\r\n",
        "### 卷积网络相关层\r\n",
        "\r\n",
        "Conv1D：普通一维卷积，常用于文本。参数个数 = 输入通道数×卷积核尺寸(如3)×卷积核个数\r\n",
        "\r\n",
        "Conv2D：普通二维卷积，常用于图像。参数个数 = 输入通道数×卷积核尺寸(如3乘3)×卷积核个数\r\n",
        "\r\n",
        "Conv3D：普通三维卷积，常用于视频。参数个数 = 输入通道数×卷积核尺寸(如3乘3乘3)×卷积核个数\r\n",
        "\r\n",
        "SeparableConv2D：二维深度可分离卷积层。不同于普通卷积同时对区域和通道操作，深度可分离卷积先操作区域，再操作通道。即先对每个通道做独立卷积操作区域，再用1乘1卷积跨通道组合操作通道。参数个数 = 输入通道数×卷积核尺寸 + 输入通道数×1×1×输出通道数。深度可分离卷积的参数数量一般远小于普通卷积，效果一般也更好。\r\n",
        "\r\n",
        "DepthwiseConv2D：二维深度卷积层。仅有SeparableConv2D前半部分操作，即只操作区域，不操作通道，一般输出通道数和输入通道数相同，但也可以通过设置depth_multiplier让输出通道为输入通道的若干倍数。输出通道数 = 输入通道数 × depth_multiplier。参数个数 = 输入通道数×卷积核尺寸× depth_multiplier。\r\n",
        "\r\n",
        "Conv2DTranspose：二维卷积转置层，俗称反卷积层。并非卷积的逆操作，但在卷积核相同的情况下，当其输入尺寸是卷积操作输出尺寸的情况下，卷积转置的输出尺寸恰好是卷积操作的输入尺寸。\r\n",
        "\r\n",
        "LocallyConnected2D: 二维局部连接层。类似Conv2D，唯一的差别是没有空间上的权值共享，所以其参数个数远高于二维卷积。\r\n",
        "\r\n",
        "MaxPool2D: 二维最大池化层。也称作下采样层。池化层无可训练参数，主要作用是降维。\r\n",
        "\r\n",
        "AveragePooling2D: 二维平均池化层。\r\n",
        "\r\n",
        "GlobalMaxPool2D: 全局最大池化层。每个通道仅保留一个值。一般从卷积层过渡到全连接层时使用，是Flatten的替代方案。\r\n",
        "\r\n",
        "GlobalAvgPool2D: 全局平均池化层。每个通道仅保留一个值。\r\n",
        "\r\n",
        "### 循环网络相关层\r\n",
        "\r\n",
        "Embedding：嵌入层。一种比Onehot更加有效的对离散特征进行编码的方法。一般用于将输入中的单词映射为稠密向量。嵌入层的参数需要学习。\r\n",
        "\r\n",
        "LSTM：长短记忆循环网络层。最普遍使用的循环网络层。具有携带轨道，遗忘门，更新门，输出门。可以较为有效地缓解梯度消失问题，从而能够适用长期依赖问题。设置return_sequences = True时可以返回各个中间步骤输出，否则只返回最终输出。\r\n",
        "\r\n",
        "GRU：门控循环网络层。LSTM的低配版，不具有携带轨道，参数数量少于LSTM，训练速度更快。\r\n",
        "\r\n",
        "SimpleRNN：简单循环网络层。容易存在梯度消失，不能够适用长期依赖问题。一般较少使用。\r\n",
        "\r\n",
        "ConvLSTM2D：卷积长短记忆循环网络层。结构上类似LSTM，但对输入的转换操作和对状态的转换操作都是卷积运算。\r\n",
        "\r\n",
        "Bidirectional：双向循环网络包装器。可以将LSTM，GRU等层包装成双向循环网络。从而增强特征提取能力。\r\n",
        "\r\n",
        "RNN：RNN基本层。接受一个循环网络单元或一个循环单元列表，通过调用tf.keras.backend.rnn函数在序列上进行迭代从而转换成循环网络层。\r\n",
        "\r\n",
        "LSTMCell：LSTM单元。和LSTM在整个序列上迭代相比，它仅在序列上迭代一步。可以简单理解LSTM即RNN基本层包裹LSTMCell。\r\n",
        "\r\n",
        "GRUCell：GRU单元。和GRU在整个序列上迭代相比，它仅在序列上迭代一步。\r\n",
        "\r\n",
        "SimpleRNNCell：SimpleRNN单元。和SimpleRNN在整个序列上迭代相比，它仅在序列上迭代一步。\r\n",
        "\r\n",
        "AbstractRNNCell：抽象RNN单元。通过对它的子类化用户可以自定义RNN单元，再通过RNN基本层的包裹实现用户自定义循环网络层。\r\n",
        "\r\n",
        "Attention：Dot-product类型注意力机制层。可以用于构建注意力模型。\r\n",
        "\r\n",
        "AdditiveAttention：Additive类型注意力机制层。可以用于构建注意力模型。\r\n",
        "\r\n",
        "TimeDistributed：时间分布包装器。包装后可以将Dense、Conv2D等作用到每一个时间片段上。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKKVKe2ROWMH"
      },
      "source": [
        "## 二，自定义模型层\r\n",
        "如果自定义模型层没有需要被训练的参数，一般推荐使用Lamda层实现。\r\n",
        "\r\n",
        "如果自定义模型层有需要被训练的参数，则可以通过对Layer基类子类化实现。\r\n",
        "\r\n",
        "Lambda层由于没有需要被训练的参数，只需要定义正向传播逻辑即可，使用比Layer基类子类化更加简单。\r\n",
        "\r\n",
        "Lambda层的正向逻辑可以使用Python的lambda函数来表达，也可以用def关键字定义函数来表达。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnIMLmLtLQeU",
        "outputId": "bb81eb1d-da0c-4383-a1c1-332a8066f547"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "from tensorflow.keras import layers,models,regularizers\r\n",
        "\r\n",
        "mypower =layers.Lambda(lambda x:tf.math.pow(x,2))\r\n",
        "mypower(tf.range(5))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([ 0,  1,  4,  9, 16], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cByRPvdbPE64"
      },
      "source": [
        "Layer的子类化一般需要重新实现初始化方法，Build方法和Call方法。下面是一个简化的线性层的范例，类似Dense."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezw_wG6JPKo5"
      },
      "source": [
        "class Linear(layers.Layer):\r\n",
        "  def __init__(self,units = 32,**kwargs):\r\n",
        "    super(Linear,self).__init__(**kwargs)\r\n",
        "    self.units = units\r\n",
        "\r\n",
        "  #build方法一般定义Layer需要被训练的参数。\r\n",
        "  def build(self,input_shape):\r\n",
        "    self.w = self.add_weight(\"w\",shape=(input_shape[-1],self.units),initializer='random_normal',trainable=True) #注意必须要有参数名称\"w\",否则会报错\r\n",
        "    self.b = self.add_weight(\"b\",shape = (self.units,),initializer='random_normal',trainable=True)\r\n",
        "    super(Linear,self).build(input_shape) # 相当于设置self.built = True\r\n",
        "\r\n",
        "  #call方法一般定义正向传播运算逻辑，__call__方法调用了它。 \r\n",
        "  @tf.function\r\n",
        "  def call(self,inputs):\r\n",
        "    return tf.matmul(inputs,self.w)+self.b\r\n",
        "\r\n",
        "  #如果要让自定义的Layer通过Functional API 组合成模型时可以被保存成h5模型，需要自定义get_config方法。\r\n",
        "\r\n",
        "  def get_config(self):\r\n",
        "    config = super(Linear,self).get_config()\r\n",
        "    config.update({'units':self.units})\r\n",
        "    return config\r\n",
        "\r\n",
        "  \r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UltMl8d0Rvgb",
        "outputId": "b9af2ede-0637-4808-8f27-71f2685f6b9d"
      },
      "source": [
        "linear = Linear(units = 8)\r\n",
        "print(linear.built)\r\n",
        "#指定input_shape，显式调用build方法，第0维代表样本数量，用None填充\r\n",
        "linear.build(input_shape = (None,16)) \r\n",
        "print(linear.built)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BKaDIUzS0wo",
        "outputId": "86166eb6-13f2-4bc8-9e81-79a0d99dabad"
      },
      "source": [
        "linear = Linear(units = 8)\r\n",
        "print(linear.built)\r\n",
        "linear.build(input_shape = (None,16)) \r\n",
        "print(linear.compute_output_shape(input_shape = (None,16)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "(None, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fogs_bWATbJe",
        "outputId": "d5ecea29-0f66-4ade-f94c-1fbe694de346"
      },
      "source": [
        "linear = Linear(units = 16)\r\n",
        "print(linear.built)\r\n",
        "#如果built = False，调用__call__时会先调用build方法, 再调用call方法。\r\n",
        "linear(tf.random.uniform((100,64))) \r\n",
        "print(linear.built)\r\n",
        "config = linear.get_config()\r\n",
        "print(config)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "True\n",
            "{'name': 'linear_4', 'trainable': True, 'dtype': 'float32', 'units': 16}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDM0tNPUUFR1",
        "outputId": "d7d4e4bd-a2ac-4b26-93eb-ac766966258f"
      },
      "source": [
        "tf.keras.backend.clear_session()\r\n",
        "\r\n",
        "model = models.Sequential()\r\n",
        "#注意该处的input_shape会被模型加工，无需使用None代表样本数量维\r\n",
        "model.add(Linear(units = 1,input_shape = (2,)))  \r\n",
        "print(\"model.input_shape: \",model.input_shape)\r\n",
        "print(\"model.output_shape: \",model.output_shape)\r\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model.input_shape:  (None, 2)\n",
            "model.output_shape:  (None, 1)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "linear (Linear)              (None, 1)                 3         \n",
            "=================================================================\n",
            "Total params: 3\n",
            "Trainable params: 3\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdkD6TedUSRD",
        "outputId": "febe8b94-d4a6-4956-b525-2ee6d7718e20"
      },
      "source": [
        "model.compile(optimizer = \"sgd\",loss = \"mse\",metrics=[\"mae\"])\r\n",
        "print(model.predict(tf.constant([[3.0,2.0],[4.0,5.0]])))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.27856907]\n",
            " [-0.36388165]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLuG79z3Uiwr",
        "outputId": "6a2253c7-6c24-4b25-9634-69380448b0dc"
      },
      "source": [
        "# 保存成 h5模型\r\n",
        "model.save(\"linear_model.h5\",save_format = \"h5\")\r\n",
        "model_loaded_keras = tf.keras.models.load_model(\r\n",
        "    \"linear_model.h5\",custom_objects={\"Linear\":Linear})\r\n",
        "print(model_loaded_keras.predict(tf.constant([[3.0,2.0],[4.0,5.0]])))\r\n",
        "\r\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.27856907]\n",
            " [-0.36388165]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIhpgU0IUrhw",
        "outputId": "dbf40e19-7059-4dc1-c4ab-891bac2e56a2"
      },
      "source": [
        "# 保存成 tf模型\r\n",
        "model.save(\"linear_model\",save_format = \"tf\")\r\n",
        "model_loaded_tf = tf.keras.models.load_model(\"linear_model\")\r\n",
        "print(model_loaded_tf.predict(tf.constant([[3.0,2.0],[4.0,5.0]])))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: linear_model/assets\n",
            "[[-0.27856907]\n",
            " [-0.36388165]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}